{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5IgSqnfn-ZZ"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_l2wVSIn-Za"
   },
   "outputs": [],
   "source": [
    "# open the gzip files\n",
    "train = gzip.open('train-images-idx3-ubyte.gz', 'rb')\n",
    "train_label = gzip.open('train-labels-idx1-ubyte.gz', 'rb')\n",
    "test = gzip.open('t10k-images-idx3-ubyte.gz', 'rb')\n",
    "test_label = gzip.open('t10k-labels-idx1-ubyte.gz', 'rb')\n",
    "\n",
    "# reading bytes from the gzip files\n",
    "train.read(16)\n",
    "train_label.read(8)\n",
    "test.read(16)\n",
    "test_label.read(8)\n",
    "\n",
    "\n",
    "# load image data and reshape\n",
    "train_image_data = np.frombuffer(train.read(), dtype=np.uint8).reshape(-1, 28, 28)\n",
    "train_label_data = np.frombuffer(train_label.read(), dtype=np.uint8)\n",
    "\n",
    "test_image_data = np.frombuffer(test.read(), dtype=np.uint8).reshape(-1, 28, 28)\n",
    "test_label_data = np.frombuffer(test_label.read(), dtype=np.uint8)\n",
    "\n",
    "# Ensure train_image_data and train_label_data have compatible shapes\n",
    "train_image_data = train_image_data[:len(train_label_data)]\n",
    "\n",
    "# train-validation split(80-20 split)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_image_data, train_label_data, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 10 unique labels\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    index = np.where(train_labels == i)[0][0]\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(train_images[index], cmap='gray')\n",
    "    plt.title(f'Label: {i}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 : 'T-shirt'\n",
    "1 : 'Trouser',\n",
    "2 : 'Pullover',\n",
    "3 : 'Dress',\n",
    "4 : 'Coat',\n",
    "5 : 'Sandal',\n",
    "6 : 'Shirt',\n",
    "7 : 'Sneaker',\n",
    "8 : 'Bag',\n",
    "9 : 'Ankle Boot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Exploratory Data Analysis\n",
    "\n",
    "# Flatten the images\n",
    "train_images_flat = train_image_data.reshape(train_image_data.shape[0], -1)\n",
    "test_images_flat = test_image_data.reshape(test_image_data.shape[0], -1)\n",
    "\n",
    "# Create DataFrames\n",
    "train_df = pd.DataFrame(train_images_flat)\n",
    "train_df['label'] = train_label_data\n",
    "\n",
    "test_df = pd.DataFrame(test_images_flat)\n",
    "test_df['label'] = test_label_data\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution in training data\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=train_df['label'])\n",
    "plt.title('Training Data Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Visualize class distribution in test data\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=test_df['label'])\n",
    "plt.title('Test Data Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data has any nan values (Data Cleaning)\n",
    "print(np.isnan(train_images).any())\n",
    "print(np.isnan(val_images).any())\n",
    "print(np.isnan(test_image_data).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-Zmz8BZn-Zb"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images.numpy().astype(np.uint8)\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.fromarray(self.images[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_images = torch.tensor(train_images, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)  \n",
    "val_images = torch.tensor(val_images, dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "test_image_data = torch.tensor(test_image_data, dtype=torch.float32)\n",
    "test_label_data = torch.tensor(test_label_data, dtype=torch.long)\n",
    "\n",
    "# Create custom datasets with transforms (Data transformation)\n",
    "train_dataset = CustomDataset(train_images, train_labels, transform=transform_train)\n",
    "val_dataset = CustomDataset(val_images, val_labels, transform=transform_val_test)\n",
    "test_dataset = CustomDataset(test_image_data, test_label_data, transform=transform_val_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=40)\n",
    "train_images_pca = pca.fit_transform(train_images_flat)\n",
    "test_images_pca = pca.transform(test_images_flat)\n",
    "print(pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a variance of 85% with 40 PCA components. For PCA a higher variance allows us to distinguish classes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x=train_images_pca[:, 0], y=train_images_pca[:, 1], hue=train_label_data, palette='tab10')\n",
    "plt.title('PCA Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_images_pca, train_label_data)\n",
    "predictions = dt.predict(test_images_pca)\n",
    "accuracy = accuracy_score(test_label_data, predictions)\n",
    "print(f'Decision Tree Classifier Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best k\n",
    "silhouette_scores = []\n",
    "for k in range(2, 14):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(train_images_pca, train_label_data)\n",
    "    predictions = knn.predict(test_images_pca)\n",
    "    silhouette_scores.append(silhouette_score(test_images_pca, predictions))\n",
    "\n",
    "plt.plot(range(2, 14), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('KNN Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the elbow method we will pick 5 as the optimal k for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(train_images_pca, train_label_data)\n",
    "predictions = knn.predict(test_images_pca)\n",
    "accuracy = accuracy_score(test_label_data, predictions)\n",
    "print(f'KNN Classifier Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPItzuEIn-Zc"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tBS3TdrpB1X"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0DDChxPrOI8",
    "outputId": "8964885a-d12d-4c05-b03d-b79cfc9365c3"
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.float()\n",
    "            output = model(images)\n",
    "            val_loss += criterion(output, labels).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tValidation Loss: {:.6f} \\tValidation Accuracy: {:.2f}%'.format(epoch + 1, val_loss, 100. * correct / len(val_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float()\n",
    "        output = model(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        predictions.extend(pred)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('Test Loss: {:.6f} \\tTest Accuracy: {:.2f}%'.format(test_loss, 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "print(classification_report(test_label_data, predictions, target_names=[str(i) for i in range(10)]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_label_data, predictions)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[str(i) for i in range(10)], yticklabels=[str(i) for i in range(10)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(test_label_data, predictions, average='macro')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(test_label_data, predictions, average='macro')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(test_label_data, predictions, average='macro')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1MtMGg_n-Zc"
   },
   "source": [
    "Sources:\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "https://stackoverflow.com/questions/12067446/how-many-principal-components-to-take"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
